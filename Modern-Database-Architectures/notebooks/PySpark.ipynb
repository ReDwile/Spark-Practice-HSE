{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Init"
      ],
      "metadata": {
        "id": "FVYDCatCk51o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaIqKTzxAM0B"
      },
      "outputs": [],
      "source": [
        "!sudo apt update\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Добавляем Hive таблицу employee\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "       .builder \\\n",
        "       .master('local[*]') \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .enableHiveSupport() \\\n",
        "       .getOrCreate()\n",
        "spark.read.parquet('employee.parquet').repartition(1).write.saveAsTable(\"employee\")\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "NaOXEGaqI-6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Старт сессии и импорт"
      ],
      "metadata": {
        "id": "urpm38BrlEBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем необходимые библиотеки\n",
        "\n",
        "Документация PySpark: https://spark.apache.org/docs/latest/api/python/index.html\n",
        "\n",
        "Документация PySpark SQL Functions: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html"
      ],
      "metadata": {
        "id": "VbDr3R-IUhBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F"
      ],
      "metadata": {
        "id": "QFSDeF56I_C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание спарк сессии\n",
        "spark = SparkSession \\\n",
        "       .builder \\\n",
        "       .master('local[*]') \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .enableHiveSupport() \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "UX6J_dbDBCBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Input Data\n",
        "\n",
        "1) Создать собственный DataFrame\n",
        "\n",
        "2) Считать из Hive-таблицы\n",
        "\n",
        "3) Считать из файла любого формата"
      ],
      "metadata": {
        "id": "4YW5VV-HZx2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем собственный DataFrame\n",
        "sample_data = [\n",
        "    {\"name\": \"John D.\",   \"age\": 30},\n",
        "    {\"name\": \"Alice G.\",  \"age\": 25},\n",
        "    {\"name\": \"Bob T.\",    \"age\": 35},\n",
        "    {\"name\": \"EvecA.\",    \"age\": 28}\n",
        "    ]\n",
        "\n",
        "sample_df = spark.createDataFrame(sample_data)\n",
        "sample_df.show()"
      ],
      "metadata": {
        "id": "usq9rUTYZ5mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ncl4dToOaiCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Читает DataFrame из Hive-таблицы"
      ],
      "metadata": {
        "id": "2SWASe12pZlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Смотрим, какие таблицы есть в Hive metastore\n",
        "spark.sql(\"show tables\").show()"
      ],
      "metadata": {
        "id": "XuQ8ZkDSpYWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "employee_h_df = spark.read.table(\"employee\")\n",
        "employee_h_df"
      ],
      "metadata": {
        "id": "sz5YoseqpszL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "employee_h_df.show(5)"
      ],
      "metadata": {
        "id": "G1TbL6R9p4qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ELvJXqNmpYL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Читает DataFrame из файла parquet"
      ],
      "metadata": {
        "id": "CFUD69DJotf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Смотрим, какие файлы лежат в нашей директории (linux-командой)\n",
        "!ls -la | grep employee"
      ],
      "metadata": {
        "id": "khEQdMIvoxd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Читаем parquet-файл\n",
        "employee_p_df = spark.read.parquet('employee.parquet')\n",
        "employee_p_df"
      ],
      "metadata": {
        "id": "tXn4uYF1o1Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "employee_p_df.show(5)"
      ],
      "metadata": {
        "id": "g4KcqDUbo1Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L8dJKGT1o1Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Читает DataFrame из файла csv"
      ],
      "metadata": {
        "id": "Lvj2NYBCkOt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Смотрим, какие файлы лежат в нашей директории (linux-командой)\n",
        "!ls -la | grep employee"
      ],
      "metadata": {
        "id": "iy_oMHpMJHDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Читаем первые 10 строк файла (linux-командой)\n",
        "!head employee_data.csv"
      ],
      "metadata": {
        "id": "HIQNCWCdJLRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Определяем схему для витрины \"Сотрудники\"\n",
        "schema = StructType([\n",
        "    StructField(\"Id\", IntegerType(), False),  # Уникальный идентификатор\n",
        "    StructField(\"LastName\", StringType(), False),  # Фамилия\n",
        "    StructField(\"FirstName\", StringType(), False),  # Имя\n",
        "    StructField(\"MiddleName\", StringType(), True),  # Отчество\n",
        "    StructField(\"Gender\", StringType(), False),  # Пол\n",
        "    StructField(\"Age\", IntegerType(), False),  # Возраст\n",
        "    StructField(\"Department\", StringType(), False),  # Отдел\n",
        "    StructField(\"Position\", StringType(), False),  # Должность\n",
        "    StructField(\"Grade\", StringType(), False),  # Грейд\n",
        "    StructField(\"HireDate\", DateType(), False),  # Дата найма\n",
        "    StructField(\"OfferAmount\", DoubleType(), False),  # Сумма оффера\n",
        "    StructField(\"Education\", StringType(), False),  # Образование\n",
        "    StructField(\"CurrentStatus\", StringType(), False)  # Статус сотрудника\n",
        "])\n",
        "\n",
        "# Читаем данные из CSV файла\n",
        "employee_df = spark.read.csv('employee.csv', schema=schema, header=True)\n",
        "employee_df"
      ],
      "metadata": {
        "id": "S5hkG5EeIvrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Показать схему DataFrame\n",
        "employee_df.printSchema()"
      ],
      "metadata": {
        "id": "GEoaeQ4Oj7lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вывести 5 строк (не сокращая)\n",
        "employee_df.show(5, False)"
      ],
      "metadata": {
        "id": "bw0UNcayIwLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вывести основные метрики по Int атрибутам\n",
        "employee_df.select(\"Id\", \"Age\", \"OfferAmount\").describe().show()"
      ],
      "metadata": {
        "id": "_zhuivOzIwIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CI4gSGAkfFdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Переводим DF в temp view (для обращения через spark.sql)\n",
        "employee_df.createOrReplaceTempView(\"employee_df\")"
      ],
      "metadata": {
        "id": "WNa7z3taexiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Посмотрим, появилась ли таблица в Hive metastore\n",
        "spark.sql(\"show tables\").show()"
      ],
      "metadata": {
        "id": "DpaA9Fk0qBK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Boa7PX_7qMhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выведем select всех записей\n",
        "spark.sql(\"select * from employee\").show()"
      ],
      "metadata": {
        "id": "cYfDxBvtezQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выведем ФИО и позицию только активных сотрудников\n",
        "# employee_df.filter(\"CurrentStatus = 'Активен'\").select(\"LastName\", \"FirstName\", \"MiddleName\", \"Position\").show(10, False)\n",
        "spark.sql(\"\"\"\n",
        "select\n",
        "  LastName,\n",
        "  FirstName,\n",
        "  MiddleName,\n",
        "  Position\n",
        "from employee\n",
        "where CurrentStatus = 'Активен'\n",
        "          \"\"\").show(10, False)"
      ],
      "metadata": {
        "id": "crccSd5FfTeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выведем количество уволенных и активных сотрудников\n",
        "# employee_df.groupBy(\"CurrentStatus\").agg(F.count(\"LastName\").alias(\"Count\")).show()\n",
        "spark.sql(\"\"\"\n",
        "select\n",
        "  CurrentStatus,\n",
        "  count(*) as Count\n",
        "from employee\n",
        "group by CurrentStatus\n",
        "          \"\"\").show()"
      ],
      "metadata": {
        "id": "NYK6VzySgU5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вывести среднюю зп (OfferAmount) у Активных сотрудников в зависимости от Grade (округлить до 2 знаков после запятой)\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "          \"\"\").show()"
      ],
      "metadata": {
        "id": "H-TTUWrShrka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Посчитать средний возраст всех сотрудников.\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "          \"\"\").show()"
      ],
      "metadata": {
        "id": "D4JBZYFKhrnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Посчитать количество мужчин и женщин в компании.\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "          \"\"\").show()"
      ],
      "metadata": {
        "id": "wkbG7kDwhrp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Посчитать распределение сотрудников по отделам (Department).\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "          \"\"\").show()"
      ],
      "metadata": {
        "id": "7o7SHacdhrsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4*) Оценить уровень текучести кадров, посчитав процент уволенных сотрудников.\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "          \"\"\").show()"
      ],
      "metadata": {
        "id": "cEHXsCZ6J7ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Определить, как долго сотрудники работают в компании, используя разницу между текущей датой и датой их найма.\n",
        "# from pyspark.sql.functions import current_date, datediff\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "          \"\"\").show()"
      ],
      "metadata": {
        "id": "-xR-TVmwrITJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Определить средний доход специалистов в зависимости от уровня образования\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "          \"\"\").show()"
      ],
      "metadata": {
        "id": "2-npJMGhrIQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Посчитать среднюю продолжительность работы сотрудников в компании (с учетом даты найма).\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "          \"\"\").show()"
      ],
      "metadata": {
        "id": "0IL8opg6rIOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Преобразовать CurrentStatus в Int (Уволен = 0, Активен = 1)\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "          \"\"\").show()"
      ],
      "metadata": {
        "id": "RVeeOTCCrIMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MJlVZKoJrIKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v4-kLAy8rIHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gRonPc1brIA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zF-Qnk5Qte4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Новый DataFrame (Attendance)"
      ],
      "metadata": {
        "id": "_uJG_-2Mvbyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la | grep attendance"
      ],
      "metadata": {
        "id": "l1dqljEjte7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Считать DataFrame attendance.parquet\n"
      ],
      "metadata": {
        "id": "hz6Tmf7CxjSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) Посчитать среднее количество отработанных часов для всех сотрудников.\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "          \"\"\").show()"
      ],
      "metadata": {
        "id": "amngv9Zfxnsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10) Посчитать долю сотрудников, которые в среднем работают больше 8 часов в день.\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "          \"\"\").show()"
      ],
      "metadata": {
        "id": "BSatMX2oxp2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W_7Fnkjyyg99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mrgQITy8yg2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DYVB2q5Tygts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b1uZAwPNtx0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"age\").count().explain(True)"
      ],
      "metadata": {
        "id": "mL7SLd26AQF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p5pOL97PAQIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оконные функции"
      ],
      "metadata": {
        "id": "yPuzteZN34UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "windowSpec = Window.partitionBy(\"course\").orderBy(\"income\")\n",
        "df1.withColumn(\"rn\", F.row_number().over(windowSpec)).show()"
      ],
      "metadata": {
        "id": "cgzpvtDP3uFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XAFD-MJq382t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Udf-функции"
      ],
      "metadata": {
        "id": "8e1YtYkR4ZeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@udf(returnType=StringType())\n",
        "def upperCase(str):\n",
        "    return str.upper()\n",
        "\n",
        "df.withColumn(\"Cureated Name\", upperCase(col(\"Name\"))).show(truncate=False)"
      ],
      "metadata": {
        "id": "WYCvuOLQ3t3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EjPRJuZl4bLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When-конструктор"
      ],
      "metadata": {
        "id": "X0xSYw2d8JU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(covid_df\n",
        "  .select('date', ‘location',\n",
        "    when(col('total_cases') > 10000, 'Red zone')\n",
        "    .when((col('total_cases') <= 10000) & (col('total_cases') > 5000), 'Yellow zone')\n",
        "    .otherwise('Green zone').alias('zone'))\n",
        "  .show())"
      ],
      "metadata": {
        "id": "VU9OlSfI8LVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-E0DvdH8I-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "hRCeAp6MAQMM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}