{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spark RDD\n",
        "В **Apache Spark** существует интерфейс - RDD API. В нём производится работа с RDD напрямую."
      ],
      "metadata": {
        "id": "Fj6htWL-UJip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>RDD </b>(resilent distrubuted dataset) - это фундаментальная структура данных Spark, которая представляет собой неизменяемый набор данных, который вычисляются и располагается на разных узлах кластера."
      ],
      "metadata": {
        "id": "_SLt2QtaUcE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
        "* [Документация](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html)"
      ],
      "metadata": {
        "id": "YtlOMhXCUv67"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgqAPHSuJ1Eg",
        "outputId": "350a8a77-cea3-42db-dcde-867eea405883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (17.0.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.11/dist-packages (from pyarrow) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pyspark pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Вариант 1. Создаем spark-сессию, явно указывая ее параметры\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "\"\"\"\n",
        "conf = (\n",
        "    SparkConf()\n",
        "        .set('spark.ui.port', '4050')\n",
        "        .setMaster('local[*]')\n",
        ")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession(sc)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "O0QnpaPvU5Zg",
        "outputId": "8545b447-38c9-4e85-832b-7cbdb7ee653a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nconf = (\\n    SparkConf()\\n        .set('spark.ui.port', '4050')\\n        .setMaster('local[*]')\\n)\\nsc = SparkContext(conf=conf)\\nspark = SparkSession(sc)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Вариант 2. Создаем дефолтную spark-сессию\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Создаем SparkSession\n",
        "spark = SparkSession.builder.appName(\"RDD_Example\").getOrCreate()"
      ],
      "metadata": {
        "id": "V5WREx3mYWti"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для того, чтобы создать RDD необходимо к некоторой коллекции объектов применимеить операциюю parallelize. В результате работы spark разобъёт данные на куски (партиции) и отправит её части на разные worker ноды."
      ],
      "metadata": {
        "id": "QLSVxG1IVHlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем RDD из списка\n",
        "data = [(\"Иван\", 30), (\"Мария\", 25), (\"Алексей\", 35), (\"Елена\", 40)]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# Выведем данные RDD\n",
        "print(rdd.collect())  # [('Иван', 30), ('Мария', 25), ('Алексей', 35), ('Елена', 40)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I_IGi0WUI3B",
        "outputId": "79684c81-7205-4d31-fc47-a9a6739856bf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Иван', 30), ('Мария', 25), ('Алексей', 35), ('Елена', 40)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на количество партиций."
      ],
      "metadata": {
        "id": "OZJnf0roVKHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciRYjqFKUI5V",
        "outputId": "fb1fc359-c164-4b60-8a29-dc8537c9d838"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark выбирает количество партиций для каждого RDD в зависимости от конфигруции кластера - по количеству доступных вычислительных ресурсов CPU. Но можно менять количество партиций, как в большую так и в меньшую сторону. Для того чтобы поменять количество партиций в существует два метода:\n",
        "\n",
        "*   repartition - позволяет как увеличивать, так и уменьшать количество партиций, но производит полную перетасовку (shuffle) данных между узлами\n",
        "*   coelesce - метод, с помощью которого можно только уменьшать размер партций. Не требует полной перетасовки данных между исполнителями, поэтому более эффективен.\n",
        "\n"
      ],
      "metadata": {
        "id": "TTuwZDB5VMmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repartitioned = rdd.repartition(5)\n",
        "print(f\"Num partitions after repartition: {repartitioned.getNumPartitions()}\")\n",
        "\n",
        "coelesced = repartitioned.coalesce(2)\n",
        "print(f\"Num partitions after coelesce: {coelesced.getNumPartitions()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V87Q2k2QUI7p",
        "outputId": "f81139d6-1e65-4a84-995c-338ee771a3c4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num partitions after repartition: 5\n",
            "Num partitions after coelesce: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обратной операцией к операции parallelize является метод collect, который наоборот создаёт коллекцию из данных, хранящихся на различных worker нодах."
      ],
      "metadata": {
        "id": "rvncFLueVY5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUsqonsPMcAp",
        "outputId": "b20f3880-4192-470f-fce1-05e8e676bc94"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Иван', 30), ('Мария', 25), ('Алексей', 35), ('Елена', 40)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# К элементам RDD можно применять различные операции\n",
        "# Допустим, мы хотим увеличить возраст каждого человека на 1 год\n",
        "rdd_transformed = rdd.map(lambda x: (x[0], x[1] + 1))\n",
        "print(rdd_transformed.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYvRsCSkMcC3",
        "outputId": "797ae7a8-fc35-4a69-b3ee-1ee0fca0fb88"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Иван', 31), ('Мария', 26), ('Алексей', 36), ('Елена', 41)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Оставим только людей старше 30 лет\n",
        "rdd_filtered = rdd.filter(lambda x: x[1] > 30)\n",
        "print(rdd_filtered.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF-h5fBrW7UR",
        "outputId": "b87e2b92-c277-48a2-95a4-185e3562ec5e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Алексей', 35), ('Елена', 40)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Отсортируем людей во возрасту\n",
        "rdd.sortByKey(keyfunc=lambda x: x[1]).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xED7qGKfZEgK",
        "outputId": "22262a00-5b56-4fa6-d907-83a1f7bf169c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Мария', 25), ('Иван', 30), ('Алексей', 35), ('Елена', 40)]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Отсортируем людей по именам\n",
        "rdd.sortByKey().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj-qUbegXA4z",
        "outputId": "6b3abbb3-ee18-4452-8ece-281074e5f64d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Алексей', 35), ('Елена', 40), ('Иван', 30), ('Мария', 25)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MPjkPxF_aHqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Допустим, у нас есть повторяющиеся имена, и мы хотим найти их средний возраст\n",
        "\n",
        "data_with_duplicates = [(\"Иван\", 30), (\"Мария\", 25), (\"Иван\", 40), (\"Мария\", 35)]\n",
        "rdd_with_dup = spark.sparkContext.parallelize(data_with_duplicates)"
      ],
      "metadata": {
        "id": "9XxCc1SQbUl3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_with_dup.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfAczcz_bVQg",
        "outputId": "761585e0-5474-47e8-ad76-025042aad2fe"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Иван', 30), ('Мария', 25), ('Иван', 40), ('Мария', 35)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_grouped = rdd_with_dup.groupByKey()"
      ],
      "metadata": {
        "id": "nUM9wRM1b-TC"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_grouped.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g9A0YaubY_u",
        "outputId": "33f22342-6f13-44af-97bd-b913be83e4df"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Иван', <pyspark.resultiterable.ResultIterable at 0x7c699f390950>),\n",
              " ('Мария', <pyspark.resultiterable.ResultIterable at 0x7c699f390210>)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[(key, list(values)) for key, values in rdd_grouped.collect()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4x2pCT7b7V6",
        "outputId": "825bda03-c066-4095-be46-1ebbcd539718"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Иван', [30, 40]), ('Мария', [25, 35])]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Хотим найти сумму возраста повторяющихся имен\n",
        "rdd_with_dup.groupByKey().mapValues(sum).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vJTt3hphbfdl",
        "outputId": "44d3f8a1-d376-421b-a0b8-f74a5d8d3183"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Иван', 70), ('Мария', 60)]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Хотим найти количество повторяющихся имен\n",
        "rdd_with_dup.groupByKey().mapValues(lambda x: len(list(x))).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-FuDSmGcVT3",
        "outputId": "1e96d30e-0163-4b9b-83a2-f24557be8bb1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Иван', 2), ('Мария', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Хотим найти средний возраст повторяющихся имен\n",
        "rdd_with_dup.groupByKey().mapValues(lambda x: sum(list(x)) / len(list(x))).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8o2U8PjTc4A-",
        "outputId": "b5c16ff3-ebc2-435d-f4cb-1d0ae8722249"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Иван', 35.0), ('Мария', 30.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3W0_m0kjXac8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD API поддердивает прямую загрузку из текстового файла. При этом каждая строка будет интерпретироваться, как отдельный элемент RDD"
      ],
      "metadata": {
        "id": "hG-ts0VnVm-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! echo \"Hello, sample RDD\" > text.txt\n",
        "! echo \"This RDD contains three lines\" >> text.txt\n",
        "! echo \"This is the last line\" >> text.txt\n",
        "! echo \"\" >> text.txt\n",
        "! echo \"Just kidding, it contains five lines\" >> text.txt"
      ],
      "metadata": {
        "id": "FynlZXvwVsft"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = sc.textFile('text.txt')\n",
        "text_data, text_data.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01REIBq1McJN",
        "outputId": "f83e97a6-b530-438d-b44b-430fd0d0a10a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(text.txt MapPartitionsRDD[140] at textFile at NativeMethodAccessorImpl.java:0,\n",
              " ['Hello, sample RDD',\n",
              "  'This RDD contains three lines',\n",
              "  'This is the last line',\n",
              "  '',\n",
              "  'Just kidding, it contains five lines'])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь к данному RDD можно применять стандартные Spark операции"
      ],
      "metadata": {
        "id": "xDpBC-EMVyBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_words = (\n",
        "    text_data\n",
        "        .filter(lambda x: len(x)) # отбираем только не пустые строки\n",
        "        .flatMap(lambda x: x.split(' ')) # разбиваем все строки на слова и переводим список\n",
        "        .distinct() # берём только уникальные слова\n",
        ")\n",
        "# Будьте внимательны, если такой файл существует,\n",
        "# то spark будет выдавать ошибку\n",
        "distinct_words.saveAsTextFile('words.txt')\n",
        "\n",
        "# можно вывести отладочную информацию по данному RDD\n",
        "print(distinct_words.toDebugString().decode())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "Akz6QBSJMcLu",
        "outputId": "4323fcc5-2088-49cc-e9a5-51312ace7b60"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o899.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/content/words.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-89f2b19c0dba>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Будьте внимательны, если такой файл существует,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# то spark будет выдавать ошибку\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdistinct_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'words.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# можно вывести отладочную информацию по данному RDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   3423\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3424\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3425\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3427\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o899.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/content/words.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Снизу вверх показаны все низкоуровневые операции (lineage), которые были применены к данному RDD c самого начала его создания"
      ],
      "metadata": {
        "id": "gjAdSOHbV5bG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4dIOUvAcV5yt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для того, чтобы переиспользовать посчитанные значения в рамках текущей сессии стоит использовать метод `.cache`, который сохраняет результат вычислений вершины графа вычислений в оперативной памяти. Это нужно для того, чтобы оперции, работающие поверх данной получали результат операций из оперативной памяти, а не считывались с диска.\n",
        "\n",
        "Метод `.persist` позволяет сохранять промежуточные вычисления в рамках текущей сессии с более тонкой настройкой места хранения (жёсткий диск, оперативная память, ...)"
      ],
      "metadata": {
        "id": "5fiQ37ApV_X2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_words_cached = distinct_words.cache()\n",
        "print(distinct_words_cached.toDebugString().decode())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGZjYtP_Vzll",
        "outputId": "d08e55b7-df52-4742-dc66-7307bca20d25"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2) PythonRDD[22] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\n",
            " |  MapPartitionsRDD[18] at mapPartitions at PythonRDD.scala:160 [Memory Serialized 1x Replicated]\n",
            " |  ShuffledRDD[17] at partitionBy at NativeMethodAccessorImpl.java:0 [Memory Serialized 1x Replicated]\n",
            " +-(2) PairwiseRDD[16] at distinct at <ipython-input-12-89f2b19c0dba>:5 [Memory Serialized 1x Replicated]\n",
            "    |  PythonRDD[15] at distinct at <ipython-input-12-89f2b19c0dba>:5 [Memory Serialized 1x Replicated]\n",
            "    |  text.txt MapPartitionsRDD[14] at textFile at NativeMethodAccessorImpl.java:0 [Memory Serialized 1x Replicated]\n",
            "    |  text.txt HadoopRDD[13] at textFile at NativeMethodAccessorImpl.java:0 [Memory Serialized 1x Replicated]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как можно заметить после операции cache появились дополнительные вершины в графе вычислений, в которых указаны место расположения данных и количество их реплик: Memory Serialized 1x Replicated"
      ],
      "metadata": {
        "id": "l8mNtaiWWFDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_words_cached.collect()\n",
        "print(distinct_words_cached.toDebugString().decode())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUH5-unJV0LO",
        "outputId": "88810855-ea8d-4b89-f4b0-f8b7a0fd8605"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2) PythonRDD[22] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\n",
            " |       CachedPartitions: 2; MemorySize: 296.0 B; DiskSize: 0.0 B\n",
            " |  MapPartitionsRDD[18] at mapPartitions at PythonRDD.scala:160 [Memory Serialized 1x Replicated]\n",
            " |  ShuffledRDD[17] at partitionBy at NativeMethodAccessorImpl.java:0 [Memory Serialized 1x Replicated]\n",
            " +-(2) PairwiseRDD[16] at distinct at <ipython-input-12-89f2b19c0dba>:5 [Memory Serialized 1x Replicated]\n",
            "    |  PythonRDD[15] at distinct at <ipython-input-12-89f2b19c0dba>:5 [Memory Serialized 1x Replicated]\n",
            "    |  text.txt MapPartitionsRDD[14] at textFile at NativeMethodAccessorImpl.java:0 [Memory Serialized 1x Replicated]\n",
            "    |  text.txt HadoopRDD[13] at textFile at NativeMethodAccessorImpl.java:0 [Memory Serialized 1x Replicated]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для сохранения данных между сессиями можно использовать `.checkpoint`. Особенность этого метода — изменение графа вычислений.\n",
        "Цепочка вычислений для сохраняемого RDD будет удалена.\n",
        "\n",
        "Сокращение цепочки вычислений полезно в случае больших графов, например, в итеративных алгоритмах."
      ],
      "metadata": {
        "id": "eJByVn4oWMGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_first_words = (\n",
        "    text_data\n",
        "        .filter(lambda x: len(x))\n",
        "        .flatMap(lambda x: x.split(' ')[0])\n",
        "        .distinct()\n",
        ")\n",
        "\n",
        "sc.setCheckpointDir('./checkpoints')\n",
        "\n",
        "distinct_first_words.checkpoint()\n",
        "print(distinct_first_words.toDebugString().decode())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J1jC39eV0JF",
        "outputId": "10344c7f-75ee-4af3-d73c-186c735d16c4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2) PythonRDD[27] at RDD at PythonRDD.scala:53 []\n",
            " |  MapPartitionsRDD[26] at mapPartitions at PythonRDD.scala:160 []\n",
            " |  ShuffledRDD[25] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
            " +-(2) PairwiseRDD[24] at distinct at <ipython-input-15-b8ebd5c157fb>:5 []\n",
            "    |  PythonRDD[23] at distinct at <ipython-input-15-b8ebd5c157fb>:5 []\n",
            "    |  text.txt MapPartitionsRDD[14] at textFile at NativeMethodAccessorImpl.java:0 []\n",
            "    |  text.txt HadoopRDD[13] at textFile at NativeMethodAccessorImpl.java:0 []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_first_words.collect()\n",
        "print(distinct_first_words.toDebugString().decode())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixzLxOdDV0G6",
        "outputId": "a156c278-b9b6-4d1a-fc3a-7a55a3d70615"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2) PythonRDD[27] at RDD at PythonRDD.scala:53 []\n",
            " |  ReliableCheckpointRDD[28] at collect at <ipython-input-16-c1aaa7e99388>:1 []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как можно увидеть предыдущий граф вычилений был полность удалён, и теперь вычисления начинаются с загрузки контрольной точки: ReliableCheckpointRDD"
      ],
      "metadata": {
        "id": "WBusyCo9WS3P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7YNihzVEV0Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KA3b7DoRd2r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Практика"
      ],
      "metadata": {
        "id": "5s7QVPxAemJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Есть файл google_queries.csv, в нем указаны запросы пользователей в Google, количесво данных запросов в день, день запроса"
      ],
      "metadata": {
        "id": "SubGXT4zeo4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head google_queries.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt7YY0f7d2pg",
        "outputId": "a432d03b-6e9a-44a7-9242-411dd38e7c01"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "request,number_of_views,date\r\n",
            "карта,3,2025-02-11\r\n",
            "погода,5,2025-02-13\r\n",
            "котики,5,2025-02-11\r\n",
            "игры,2,2025-02-14\r\n",
            "погода,1,2025-02-17\r\n",
            "ютуб,1,2025-02-13\r\n",
            "погода,1,2025-02-18\r\n",
            "вк,1,2025-02-15\r\n",
            "авито,5,2025-02-11\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Создаем SparkSession\n",
        "spark = SparkSession.builder.appName(\"GoogleQueriesAnalysis\").getOrCreate()\n",
        "sc = spark.sparkContext  # Получаем SparkContext\n",
        "\n",
        "# Загружаем файл в RDD\n",
        "rdd = sc.textFile(\"google_queries.csv\")"
      ],
      "metadata": {
        "id": "IYDoyGyHe03S"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOkjEwZ_fAJg",
        "outputId": "640f3128-e5cc-45cb-e2c4-8d33dc257199"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['request,number_of_views,date',\n",
              " 'карта,3,2025-02-11',\n",
              " 'погода,5,2025-02-13',\n",
              " 'котики,5,2025-02-11',\n",
              " 'игры,2,2025-02-14']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Пропускаем заголовок\n",
        "header = rdd.first()\n",
        "rdd = rdd.filter(lambda line: line != header)"
      ],
      "metadata": {
        "id": "Rz9AB3J7e6vi"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8j0NUKkfFIA",
        "outputId": "2698eacf-0470-4874-f890-28223e6d4f54"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['карта,3,2025-02-11',\n",
              " 'погода,5,2025-02-13',\n",
              " 'котики,5,2025-02-11',\n",
              " 'игры,2,2025-02-14',\n",
              " 'погода,1,2025-02-17']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Парсим данные (request, number_of_views, date)\n",
        "rdd_parsed = rdd.map(lambda line: line.split(\",\")).map(lambda x: (x[0], int(x[1]), x[2]))"
      ],
      "metadata": {
        "id": "Qs80FIdde8hK"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_parsed.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7eG2_vjfJDn",
        "outputId": "53cc8ef9-61c8-46cd-9c48-c21d85ba045d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('карта', 3, '2025-02-11'),\n",
              " ('погода', 5, '2025-02-13'),\n",
              " ('котики', 5, '2025-02-11'),\n",
              " ('игры', 2, '2025-02-14'),\n",
              " ('погода', 1, '2025-02-17')]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qop48eFEf86W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Общее количество запросов\n",
        "total_queries = rdd_parsed.count()\n",
        "print(f\"1. Общее количество запросов: {total_queries}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb1h5OArfO9i",
        "outputId": "32da8ed8-5d27-45e3-825f-3d71ee45405f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Общее количество запросов: 255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MfuWNyRQf9XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Общее количество просмотров\n",
        "total_views = rdd_parsed.map(lambda x: x[1]).sum()\n",
        "print(f\"2. Общее количество просмотров: {total_views}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU5dBKY7fUip",
        "outputId": "40a5aff8-8435-4162-aa69-515ca6d5d290"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2. Общее количество просмотров: 743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C4Bem1Dof973"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Среднее количество просмотров на запрос\n",
        "avg_views_per_query = total_views / total_queries if total_queries > 0 else 0\n",
        "print(f\"3. Среднее количество просмотров на запрос: {avg_views_per_query:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srjLfdRffaaA",
        "outputId": "df480e6f-cca4-4d98-91f5-ac550a0926df"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3. Среднее количество просмотров на запрос: 2.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6LIkBwJnf-j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Запрос с максимальным числом просмотров\n",
        "max_request = rdd_parsed.max(key=lambda x: x[1])\n",
        "print(f\"4. Запрос с максимальным числом просмотров: {max_request}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwVxBepTff22",
        "outputId": "3c42d13b-b493-451f-a0fb-e2e218e59491"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4. Запрос с максимальным числом просмотров: ('погода', 5, '2025-02-13')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ggMvYMjcf_AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Запрос с минимальным числом просмотров\n",
        "min_request = rdd_parsed.min(key=lambda x: x[1])\n",
        "print(f\"5. Запрос с минимальным числом просмотров: {min_request}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whiWUskhfkbJ",
        "outputId": "bf44bed2-2bd8-425a-9986-685e177aa891"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5. Запрос с минимальным числом просмотров: ('погода', 1, '2025-02-17')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lqPRVK8Sf_id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Количество уникальных запросов\n",
        "unique_requests = rdd_parsed.map(lambda x: x[0]).distinct().count()\n",
        "print(f\"6. Количество уникальных запросов: {unique_requests}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ87fxXffniV",
        "outputId": "ff0ad87d-648f-46b4-988e-b5ab14bfda4b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6. Количество уникальных запросов: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hrY-7JEuf__d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Среднее количество просмотров на дату\n",
        "views_per_date = rdd_parsed.map(lambda x: (x[2], x[1])).groupByKey().mapValues(lambda values: sum(values) / len(values))\n",
        "avg_views_per_date = views_per_date.map(lambda x: x[1]).mean()\n",
        "print(f\"7. Среднее количество просмотров на дату: {avg_views_per_date:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W24admZ_frVy",
        "outputId": "1ecde0e8-ca6b-4c34-e169-eb7dfc112679"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7. Среднее количество просмотров на дату: 2.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a83PpCWRgAcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Общее число дат, когда были поисковые запросы\n",
        "unique_dates = rdd_parsed.map(lambda x: x[2]).distinct().count()\n",
        "print(f\"8. Общее число дат, когда были поисковые запросы: {unique_dates}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1twpjkeHfvvh",
        "outputId": "c71d04bf-7006-4d93-b222-93df5233563d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8. Общее число дат, когда были поисковые запросы: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zWEYRDM0gA4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Запрос с наибольшим числом дней появления\n",
        "query_day_count = rdd_parsed.map(lambda x: (x[0], x[2])).distinct().map(lambda x: (x[0], 1)) \\\n",
        "                            .reduceByKey(lambda a, b: a + b)\n",
        "max_days_request = query_day_count.max(key=lambda x: x[1])\n",
        "print(f\"9. Запрос с наибольшим числом дней появления: {max_days_request}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBB_OtNjfzSt",
        "outputId": "1031a638-a1bf-4db2-9701-666cf0f76a83"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9. Запрос с наибольшим числом дней появления: ('фильмы', 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L-vni5dDgBUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Самая популярная дата по просмотрам\n",
        "popular_date = rdd_parsed.map(lambda x: (x[2], x[1])).reduceByKey(lambda a, b: a + b) \\\n",
        "                         .max(key=lambda x: x[1])\n",
        "print(f\"10. Самая популярная дата по просмотрам: {popular_date}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd5QGvusf3Eg",
        "outputId": "ac26fc60-8056-452e-c6ea-b4090f799ade"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10. Самая популярная дата по просмотрам: ('2025-02-11', 97)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T-EbVoQQd2lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Дата с наибольшим количеством уникальных запросов\n",
        "unique_queries_per_date = rdd_parsed.map(lambda x: (x[2], x[0])).distinct() \\\n",
        "                                    .map(lambda x: (x[0], 1)).reduceByKey(lambda a, b: a + b)\n",
        "max_unique_query_date = unique_queries_per_date.max(key=lambda x: x[1])\n",
        "print(f\"11. Дата с наибольшим количеством уникальных запросов: {max_unique_query_date}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoWCzBsCV0CP",
        "outputId": "485202ab-c314-41b3-834f-f3fd498ba3cd"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11. Дата с наибольшим количеством уникальных запросов: ('2025-02-11', 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "grqUqAe7ipYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Общее количество дней, когда было более 3 уникальных запросов\n",
        "days_with_more_than_3_queries = unique_queries_per_date.filter(lambda x: x[1] > 3).count()\n",
        "print(f\"12. Общее количество дней с >3 уникальными запросами: {days_with_more_than_3_queries}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QP6psAD7ipVr",
        "outputId": "e10a9af5-bfb7-4b04-ccc5-ac8f2c824053"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12. Общее количество дней с >3 уникальными запросами: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RBCpB75QipTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Топ-3 самых популярных запроса по просмотрам\n",
        "top_3_queries = rdd_parsed.map(lambda x: (x[0], x[1])).reduceByKey(lambda a, b: a + b) \\\n",
        "                          .top(3, key=lambda x: x[1])\n",
        "print(f\"13. Топ-3 самых популярных запроса по просмотрам: {top_3_queries}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ladFNkWgipQ_",
        "outputId": "c6955a4a-dc3d-4aef-9301-367386e3d83a"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13. Топ-3 самых популярных запроса по просмотрам: [('игры', 68), ('авито', 65), ('вк', 61)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2zwg7BhaioRf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}