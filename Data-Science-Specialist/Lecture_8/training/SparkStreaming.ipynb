{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "460dfb6e-7df2-451a-8fc0-302f36167879",
      "metadata": {
        "id": "460dfb6e-7df2-451a-8fc0-302f36167879"
      },
      "source": [
        "# `Промышленное машинное обучение на Spark`\n",
        "## `Занятие 08: Spark Structured Streaming`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feadc31e-7e74-488b-96bf-e69213201797",
      "metadata": {
        "id": "feadc31e-7e74-488b-96bf-e69213201797"
      },
      "source": [
        "О чём можно узнать из этого ноутбука:\n",
        "\n",
        "* Обработка данных в онлайн-режиме\n",
        "* Подключение источников\n",
        "* Spark Streaming: Filters, Join, Window"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b13b6be-28ca-4187-981e-81030844fc4d",
      "metadata": {
        "id": "2b13b6be-28ca-4187-981e-81030844fc4d"
      },
      "source": [
        "Произведём все необходимые импорты и создадим Spark-context для дальнейшей работы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "97e17c15-e216-473d-85b0-854bd86177b7",
      "metadata": {
        "id": "97e17c15-e216-473d-85b0-854bd86177b7"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql.types import StructType, StructField\n",
        "from pyspark.sql.types import StringType, DoubleType, IntegerType, LongType\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = (\n",
        "    SparkConf()\n",
        "        .set('spark.ui.port', '4050')\n",
        "        .set('spark.driver.memory', '6g')\n",
        "        .set('spark.executor.extraJavaOptions', '-Xss512m')\n",
        "        .set('spark.driver.extraJavaOptions', '-Xss512m')\n",
        "        .setMaster('local[*]')\n",
        "        .setAppName(\"StructuredNetworkWordCount\")\n",
        ")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2186deac-698e-4179-b512-804a8f9e5b5b",
      "metadata": {
        "id": "2186deac-698e-4179-b512-804a8f9e5b5b"
      },
      "source": [
        "Далее создадим специальный датафрейм, в который будут приходить данные по сети. Этот датафрейм представляет собой абстракцию таблицы с бесконечным числом строк, в котором все пришедшие данные располагаются в колонке `value`.\n",
        "\n",
        "Вызов метода `readStream` указывает на то, что созданный датафрейм будет потокового типа, также при создании необходимо указать откуда будут идти данные. Источниками данных могут быть:\n",
        "* Открытыте сетевые соединения\n",
        "* Дириктории в локальной файловой системе или hdfs\n",
        "* Из брокеров сообщений, например Kafka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c892c0fb-d403-402e-bf84-8ef14852e4e4",
      "metadata": {
        "id": "c892c0fb-d403-402e-bf84-8ef14852e4e4"
      },
      "outputs": [],
      "source": [
        "words_lines = (spark\n",
        "        .readStream\n",
        "        .format('socket') # аргумент socket указывает на то, что данные будут приходить по сети\n",
        "        .option('host', 'localhost') # указываем на каком хосте располагаются данные\n",
        "        .option('port', '10000') # указываем на каком порте нужно вычитывать их\n",
        "        .load()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e9930fc-13c1-4382-a93e-cd1407774478",
      "metadata": {
        "id": "7e9930fc-13c1-4382-a93e-cd1407774478"
      },
      "source": [
        "Сделаем вывод схемы данных и типа созданного датафрейма"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "237de728-abc5-404c-ae60-723fdc868cc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "237de728-abc5-404c-ae60-723fdc868cc0",
        "outputId": "9b7be09a-668a-456a-de9b-7b0672514027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- value: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Streaming DataFame: True'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "words_lines.printSchema()\n",
        "\n",
        "f\"Streaming DataFame: {words_lines.isStreaming}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e11f5c11-7220-4c9e-984c-117a408092e4",
      "metadata": {
        "id": "e11f5c11-7220-4c9e-984c-117a408092e4"
      },
      "source": [
        "Далее уже можно приступать непосредственно к описании требуемой логики обработки данных. В ячейке ниже происходит операция подсчёта пришедших слов, для этого разбиваем строки на списки слов и представляем каждое слово в виде отдельной строки в абстрактной таблице."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f460fb27-b989-4619-8b8f-12c7981086b3",
      "metadata": {
        "id": "f460fb27-b989-4619-8b8f-12c7981086b3"
      },
      "outputs": [],
      "source": [
        "words = words_lines.select(\n",
        "        F.explode( # преобразуем пришедшие данные в список и каждый элемент списка преобразуем в строку датафрейма\n",
        "            F.split(words_lines.value, ' ') # атрибут .value - данные которые приходят на host:port\n",
        "        ).alias('word')\n",
        "    )\n",
        "\n",
        "wordCounts = words.groupBy('word').count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fadf79a-b522-4471-961b-e1d78d67f7f4",
      "metadata": {
        "id": "9fadf79a-b522-4471-961b-e1d78d67f7f4"
      },
      "source": [
        "В предыдущих ячейках были сформировны два из трёх шагов: вычитка и процессеинг. Теперь сформируем последний шаг - запись. В нём указываем куда пишем данные - параметр метода format, и по какой логике формируется вывод - метод outputMode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9a120132-88d2-4d81-83fc-1e9850bf5a3e",
      "metadata": {
        "id": "9a120132-88d2-4d81-83fc-1e9850bf5a3e"
      },
      "outputs": [],
      "source": [
        "query = (wordCounts\n",
        "        .writeStream\n",
        "        .outputMode('complete')\n",
        "        .format('console')\n",
        "        .start()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aa22b87-31bf-4a97-995d-9ab3f6e323e0",
      "metadata": {
        "id": "7aa22b87-31bf-4a97-995d-9ab3f6e323e0"
      },
      "source": [
        "Подобно тому как исполнение операций в batch-режиме производлось после вызова action операций, в SparkStreaming вычитка новых данных происходит после запуска метода awaitTermination, в котором производится старт фоновой задачи на обновление."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5bee1d9-dd25-48f8-92c1-a40cca6012bc",
      "metadata": {
        "id": "e5bee1d9-dd25-48f8-92c1-a40cca6012bc"
      },
      "outputs": [],
      "source": [
        "query.awaitTermination()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "353ac69a-35ed-4cbc-a29a-2c9c8c79cf45",
      "metadata": {
        "id": "353ac69a-35ed-4cbc-a29a-2c9c8c79cf45"
      },
      "source": [
        "## Чтение данных из файловой системы"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6b00539-567e-4ead-ab1a-8a2c3113f7be",
      "metadata": {
        "id": "d6b00539-567e-4ead-ab1a-8a2c3113f7be"
      },
      "source": [
        "Попробуем считать данные из файловой системы, для этого рекомендуется заранее описать схему данных, а затем указать, какую директорию в файловой системе необходимо мониторить"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aa34ffe-cf39-4e34-9a02-be569f4e3656",
      "metadata": {
        "id": "1aa34ffe-cf39-4e34-9a02-be569f4e3656"
      },
      "outputs": [],
      "source": [
        "# описываем схему данных таблиц из директории data\n",
        "userSchema = StructType(\n",
        "    [\n",
        "        StructField(\"Date\", StringType(), True),\n",
        "        StructField(\"Open\", DoubleType(), True),\n",
        "        StructField(\"High\", DoubleType(), True),\n",
        "        StructField(\"Low\", DoubleType(), True),\n",
        "        StructField(\"Close\", DoubleType(), True),\n",
        "        StructField(\"Adjusted Close\", DoubleType(), True),\n",
        "        StructField(\"Volume\", DoubleType(), True),\n",
        "    ]\n",
        ")\n",
        "# Так как информация о названии компании содержится\n",
        "# в названии файла, то добавим дополнительное поле в\n",
        "# датафрейм из названия файла\n",
        "def get_company():\n",
        "    filename = F.element_at(\n",
        "        F.split(F.input_file_name(), \"/\"), -1\n",
        "    )\n",
        "    return F.element_at(F.split(filename, \"_\"), 1)\n",
        "\n",
        "initDF = (spark\n",
        "  .readStream\n",
        "  .format(\"csv\")\n",
        "  .option(\"maxFilesPerTrigger\", 2) # максимальное кол-во одновременно считываемых файлов\n",
        "  .option(\"header\", True)\n",
        "  .option(\"path\", \"sourceDir\") # путь к директории, которую необходимо отслеживать\n",
        "  .schema(userSchema)\n",
        "  .load()\n",
        "  .withColumn(\"Name\", get_company()) # добавим к колонкам ещё название компании\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06d7d256-09ec-42b0-a38d-e15e4f4cabdf",
      "metadata": {
        "id": "06d7d256-09ec-42b0-a38d-e15e4f4cabdf"
      },
      "source": [
        "Задаём логику обработки данных. Производим агргацию данных об акциях компания, где будет указана самая высокая цена акции в некотором году."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bab1c10-2d0f-4452-913f-1b264cc7da82",
      "metadata": {
        "id": "5bab1c10-2d0f-4452-913f-1b264cc7da82"
      },
      "outputs": [],
      "source": [
        "stock_df = (\n",
        "    initDF\n",
        "    .groupBy(F.col(\"Name\"), F.year(F.col(\"Date\")).alias(\"Year\"))\n",
        "    .agg(F.max(\"High\").alias(\"Max\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "138ed099-dfbc-477d-8562-04985c36f93b",
      "metadata": {
        "id": "138ed099-dfbc-477d-8562-04985c36f93b"
      },
      "source": [
        "Задаём логику записи данных в консоль."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3e9032f-8f02-4d59-9aee-2665dd3b771a",
      "metadata": {
        "id": "c3e9032f-8f02-4d59-9aee-2665dd3b771a",
        "outputId": "389d9622-002c-460e-8508-33ac3e94b683"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/12/02 00:17:17 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4b7d1092-3e3d-4470-9388-15f272a746ec. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
            "23/12/02 00:17:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
          ]
        }
      ],
      "source": [
        "query = (\n",
        "  stock_df\n",
        "  .writeStream\n",
        "  .outputMode(\"update\") # по какой логику делаем вывод - только обновления\n",
        "  .option(\"truncate\", False)\n",
        "  .format(\"console\")\n",
        "  .start()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd59abe9-4263-4c74-ba3a-52361031be89",
      "metadata": {
        "id": "bd59abe9-4263-4c74-ba3a-52361031be89"
      },
      "outputs": [],
      "source": [
        "query.awaitTermination()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1372770c-b6e2-4631-a681-13a1598716fb",
      "metadata": {
        "id": "1372770c-b6e2-4631-a681-13a1598716fb"
      },
      "source": [
        "Можно записывать обработанные данные также файл и в файл, но тут есть ограничения на производимые операции. Так как запись в файл поддерживает только тип записи `append` - данные внутри файлов можно добавлять, но не изменять."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2b839a1-1cf0-4645-84ca-c4857b552837",
      "metadata": {
        "id": "e2b839a1-1cf0-4645-84ca-c4857b552837",
        "outputId": "8be81d77-8d56-400d-fa57-dfa74415a068"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/12/02 06:56:35 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            "23/12/02 06:56:35 WARN StreamingQueryManager: Stopping existing streaming query [id=35be1d0b-031e-451b-bd48-0d58b19476c4, runId=382e6bed-abf5-4cf3-9dc1-0ae6b7216774], as a new run is being started.\n"
          ]
        }
      ],
      "source": [
        "stock_df = (\n",
        "    initDF\n",
        "    .where(F.col(\"Close\") - F.col(\"Open\") > 0)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca01fdb6-6204-4470-a818-9a95a6ad14df",
      "metadata": {
        "id": "ca01fdb6-6204-4470-a818-9a95a6ad14df"
      },
      "outputs": [],
      "source": [
        "stock_df\\\n",
        "  .writeStream\\\n",
        "  .outputMode(\"append\")\\\n",
        "  .format(\"csv\")\\\n",
        "  .option(\"path\", \"output\")\\\n",
        "  .option(\"header\", True)\\\n",
        "  .option(\"checkpointLocation\", \"checkpoints/\")\\\n",
        "  .start()\\\n",
        "  .awaitTermination()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aab9193-c3de-490d-9d6b-adb6e12ce69a",
      "metadata": {
        "id": "5aab9193-c3de-490d-9d6b-adb6e12ce69a"
      },
      "source": [
        "### Join & Windows"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9351c61-444f-4d50-9468-59b76c255cdc",
      "metadata": {
        "id": "a9351c61-444f-4d50-9468-59b76c255cdc"
      },
      "source": [
        "Потоковые данные можно джойнить между собой или же между привычными  batch-датафреймами."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69bbf39b-30f4-4c71-b504-c6711fc09e14",
      "metadata": {
        "id": "69bbf39b-30f4-4c71-b504-c6711fc09e14",
        "outputId": "7f3abe6a-9c8c-4231-8afd-870cf7be1600"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+--------+\n",
            "|Name|FullName|\n",
            "+----+--------+\n",
            "|AAPL|   Apple|\n",
            "|AMZN|  Amazon|\n",
            "+----+--------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "companyFullNames = spark.createDataFrame(\n",
        "    [\n",
        "        (\"AAPL\", \"Apple\"),\n",
        "        (\"AMZN\", \"Amazon\"),\n",
        "        (\"GOOGL\",\"Google\"),\n",
        "        (\"MSFT\", \"Microsoft\"),\n",
        "        (\"CSCO\", \"CISCO\")\n",
        "    ], [\"Name\", \"FullName\"]\n",
        ")\n",
        "\n",
        "companyFullNames.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f320f66-663f-4f17-921a-559929c3eda1",
      "metadata": {
        "id": "7f320f66-663f-4f17-921a-559929c3eda1"
      },
      "outputs": [],
      "source": [
        "stock_df = (\n",
        "    initDF\n",
        "    .groupBy(\"Name\", F.year(\"Date\").alias(\"Year\"))\n",
        "    .agg(F.max(\"High\").alias(\"Max\"))\n",
        ")\n",
        "\n",
        "stock_df = stock_df.join(companyFullNames, on=\"Name\", how=\"inner\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bca83f2b-d5f9-4ffa-bc71-7ca2e01b5490",
      "metadata": {
        "id": "bca83f2b-d5f9-4ffa-bc71-7ca2e01b5490",
        "outputId": "2ce1dee9-2f8e-4be7-d15c-ade80214cc6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/12/02 08:24:12 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5d4b566b-e312-429a-a4e3-2002f1f38d20. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
            "23/12/02 08:24:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            "                                                                                                                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+----+----+------+--------+\n",
            "|Name|Year|Max   |FullName|\n",
            "+----+----+------+--------+\n",
            "|AAPL|2007|28.99 |Apple   |\n",
            "|AAPL|2010|46.67 |Apple   |\n",
            "|AAPL|2013|82.16 |Apple   |\n",
            "|AAPL|2015|134.54|Apple   |\n",
            "|AAPL|2016|118.69|Apple   |\n",
            "|AAPL|2011|60.96 |Apple   |\n",
            "|AAPL|2009|30.56 |Apple   |\n",
            "|AAPL|2012|100.72|Apple   |\n",
            "|AAPL|2008|28.61 |Apple   |\n",
            "|AAPL|2006|13.31 |Apple   |\n",
            "+----+----+------+--------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/evgeniy/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/home/evgeniy/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[43mstock_df\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessingTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1 minute\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtruncate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumRows\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconsole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "stock_df\\\n",
        ".writeStream\\\n",
        ".outputMode(\"complete\")\\\n",
        ".trigger(processingTime='1 minute') \\\n",
        ".option(\"truncate\", False)\\\n",
        ".option(\"numRows\", 10)\\\n",
        ".format(\"console\")\\\n",
        ".start()\\\n",
        ".awaitTermination()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6f4ed94-1d55-499e-b470-cf2a22e81c1e",
      "metadata": {
        "id": "e6f4ed94-1d55-499e-b470-cf2a22e81c1e"
      },
      "source": [
        "Теперь попробуем создать оконную функцию, в которой будем обрабатывать данные, которые находятся в одном временном окне."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7688e26c-5132-4284-abce-04058673ab41",
      "metadata": {
        "id": "7688e26c-5132-4284-abce-04058673ab41",
        "outputId": "93b67bd3-d6e9-4e14-df8c-c48bd3a0773f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/12/02 08:30:13 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
            "23/12/02 08:30:13 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-8aa4f919-d12b-44a1-9993-0d6135795916. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
            "23/12/02 08:30:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            "                                                                                                                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+------+----+-----+\n",
            "|window|word|count|\n",
            "+------+----+-----+\n",
            "+------+----+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 1\n",
            "-------------------------------------------\n",
            "+------------------------------------------+-----+-----+\n",
            "|window                                    |word |count|\n",
            "+------------------------------------------+-----+-----+\n",
            "|{2023-12-02 08:29:30, 2023-12-02 08:30:30}|world|1    |\n",
            "|{2023-12-02 08:29:30, 2023-12-02 08:30:30}|hello|1    |\n",
            "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|hello|1    |\n",
            "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|world|1    |\n",
            "+------------------------------------------+-----+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 2\n",
            "-------------------------------------------\n",
            "+------------------------------------------+-----+-----+\n",
            "|window                                    |word |count|\n",
            "+------------------------------------------+-----+-----+\n",
            "|{2023-12-02 08:29:30, 2023-12-02 08:30:30}|world|1    |\n",
            "|{2023-12-02 08:29:30, 2023-12-02 08:30:30}|hello|1    |\n",
            "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|hello|2    |\n",
            "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|boys |1    |\n",
            "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|world|1    |\n",
            "|{2023-12-02 08:30:30, 2023-12-02 08:31:30}|hello|1    |\n",
            "|{2023-12-02 08:30:30, 2023-12-02 08:31:30}|boys |1    |\n",
            "+------------------------------------------+-----+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 3\n",
            "-------------------------------------------\n",
            "+------------------------------------------+---------+-----+\n",
            "|window                                    |word     |count|\n",
            "+------------------------------------------+---------+-----+\n",
            "|{2023-12-02 08:29:30, 2023-12-02 08:30:30}|world    |1    |\n",
            "|{2023-12-02 08:29:30, 2023-12-02 08:30:30}|hello    |1    |\n",
            "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|hello    |2    |\n",
            "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|boys     |1    |\n",
            "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|world    |1    |\n",
            "|{2023-12-02 08:30:30, 2023-12-02 08:31:30}|hello    |1    |\n",
            "|{2023-12-02 08:30:30, 2023-12-02 08:31:30}|boys     |1    |\n",
            "|{2023-12-02 08:31:00, 2023-12-02 08:32:00}|words    |1    |\n",
            "|{2023-12-02 08:31:00, 2023-12-02 08:32:00}|something|1    |\n",
            "|{2023-12-02 08:31:00, 2023-12-02 08:32:00}|         |1    |\n",
            "|{2023-12-02 08:31:30, 2023-12-02 08:32:30}|         |1    |\n",
            "|{2023-12-02 08:31:30, 2023-12-02 08:32:30}|something|1    |\n",
            "|{2023-12-02 08:31:30, 2023-12-02 08:32:30}|words    |1    |\n",
            "+------------------------------------------+---------+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 4\n",
            "-------------------------------------------\n",
            "+------------------------------------------+---------+-----+\n",
            "|window                                    |word     |count|\n",
            "+------------------------------------------+---------+-----+\n",
            "|{2023-12-02 08:29:30, 2023-12-02 08:30:30}|world    |1    |\n",
            "|{2023-12-02 08:29:30, 2023-12-02 08:30:30}|hello    |1    |\n",
            "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|hello    |2    |\n",
            "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|boys     |1    |\n",
            "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|world    |1    |\n",
            "|{2023-12-02 08:30:30, 2023-12-02 08:31:30}|hello    |1    |\n",
            "|{2023-12-02 08:30:30, 2023-12-02 08:31:30}|boys     |1    |\n",
            "|{2023-12-02 08:31:00, 2023-12-02 08:32:00}|words    |1    |\n",
            "|{2023-12-02 08:31:00, 2023-12-02 08:32:00}|something|1    |\n",
            "|{2023-12-02 08:31:00, 2023-12-02 08:32:00}|         |1    |\n",
            "|{2023-12-02 08:31:30, 2023-12-02 08:32:30}|         |1    |\n",
            "|{2023-12-02 08:31:30, 2023-12-02 08:32:30}|something|1    |\n",
            "|{2023-12-02 08:31:30, 2023-12-02 08:32:30}|words    |1    |\n",
            "|{2023-12-02 08:31:30, 2023-12-02 08:32:30}|world    |1    |\n",
            "|{2023-12-02 08:32:00, 2023-12-02 08:33:00}|world    |1    |\n",
            "+------------------------------------------+---------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lines = (\n",
        "    spark\n",
        "    .readStream\n",
        "    .format('socket')\n",
        "    .option('host', 'localhost')\n",
        "    .option('port', '10000')\n",
        "    .option('includeTimestamp', 'true')\n",
        "    .load()\n",
        ")\n",
        "\n",
        "words = lines.select(\n",
        "    F.explode(F.split(lines.value, ' ')).alias('word'),\n",
        "    lines.timestamp\n",
        ")\n",
        "\n",
        "windowDuration = '1 minutes'\n",
        "slideDuration = '30 seconds'\n",
        "\n",
        "windowedCounts = words.groupBy(\n",
        "    F.window(words.timestamp, windowDuration, slideDuration),\n",
        "    words.word\n",
        ").count().orderBy('window')\n",
        "\n",
        "# Start running the query that prints the windowed word counts to the console\n",
        "query = (windowedCounts\n",
        "    .writeStream\n",
        "    .outputMode('complete')\n",
        "    .format('console')\n",
        "    .option('truncate', 'false')\n",
        "    .start()\n",
        ")\n",
        "\n",
        "query.awaitTermination()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3b2cf39-e571-48af-844b-66f1b7c4b62f",
      "metadata": {
        "id": "e3b2cf39-e571-48af-844b-66f1b7c4b62f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Примеры"
      ],
      "metadata": {
        "id": "NGElC_FMVAVr"
      },
      "id": "NGElC_FMVAVr"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import from_json, col, window\n",
        "from pyspark.sql.types import StructType, StringType, TimestampType\n",
        "\n",
        "# Создание SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StructuredStreamingExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Включение логирования только ошибок\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n"
      ],
      "metadata": {
        "id": "s_GImj3LU_xh"
      },
      "id": "s_GImj3LU_xh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Чтение данных из Kafka\n",
        "raw_df = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
        "    .option(\"subscribe\", \"events_topic\") \\\n",
        "    .option(\"startingOffsets\", \"earliest\") \\\n",
        "    .load()\n"
      ],
      "metadata": {
        "id": "HtEyVphPU_vD"
      },
      "id": "HtEyVphPU_vD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Схема JSON-сообщения\n",
        "schema = StructType() \\\n",
        "    .add(\"event_type\", StringType()) \\\n",
        "    .add(\"event_time\", TimestampType())\n",
        "\n",
        "# Преобразование Kafka-сообщения\n",
        "events_df = raw_df.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
        "    .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
        "    .select(\"data.*\")\n"
      ],
      "metadata": {
        "id": "w9egat7qU_rK"
      },
      "id": "w9egat7qU_rK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Группировка по окну и типу события\n",
        "aggregated_df = events_df \\\n",
        "    .withWatermark(\"event_time\", \"10 minutes\") \\\n",
        "    .groupBy(\n",
        "        window(col(\"event_time\"), \"5 minutes\"),\n",
        "        col(\"event_type\")\n",
        "    ).count()\n"
      ],
      "metadata": {
        "id": "nHbYCLdpU_o6"
      },
      "id": "nHbYCLdpU_o6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Потоковая запись в консоль\n",
        "query = aggregated_df.writeStream \\\n",
        "    .outputMode(\"update\") \\  # используем режим update, чтобы не печатать всю таблицу каждый раз\n",
        "    .format(\"console\") \\\n",
        "    .option(\"truncate\", False) \\\n",
        "    .start()\n",
        "\n",
        "# Ожидание завершения\n",
        "query.awaitTermination()\n"
      ],
      "metadata": {
        "id": "vn6xkAw4U_m3"
      },
      "id": "vn6xkAw4U_m3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdvM5R_dU_ku"
      },
      "id": "FdvM5R_dU_ku",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример ML"
      ],
      "metadata": {
        "id": "08j81KniVO7d"
      },
      "id": "08j81KniVO7d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Подготовим модель заранее (один раз, не в стриме)\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MLModelTraining\").getOrCreate()\n",
        "\n",
        "# Обучающий датасет\n",
        "train_df = spark.createDataFrame([\n",
        "    (1, \"A\", 0.1, 0),\n",
        "    (2, \"B\", 0.3, 1),\n",
        "    (3, \"A\", 0.4, 0),\n",
        "    (4, \"B\", 0.6, 1)\n",
        "], [\"id\", \"category\", \"feature1\", \"label\"])\n",
        "\n",
        "# Индексация категориального признака\n",
        "indexer = StringIndexer(inputCol=\"category\", outputCol=\"category_indexed\")\n",
        "train_df = indexer.fit(train_df).transform(train_df)\n",
        "\n",
        "# Сбор признаков в вектор\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"category_indexed\", \"feature1\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "train_df = assembler.transform(train_df)\n",
        "\n",
        "# Обучение модели\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "model = lr.fit(train_df)\n",
        "\n",
        "# Сохраняем модель\n",
        "model.write().overwrite().save(\"/tmp/lr_model\")\n"
      ],
      "metadata": {
        "id": "-RXdvLNdVQc_"
      },
      "id": "-RXdvLNdVQc_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Стриминг и применение модели\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegressionModel\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexerModel\n",
        "\n",
        "# Spark session\n",
        "spark = SparkSession.builder.appName(\"StreamingML\").getOrCreate()\n",
        "\n",
        "# Чтение стрима CSV-файлов из папки\n",
        "schema = \"id INT, category STRING, feature1 DOUBLE\"\n",
        "stream_df = spark.readStream \\\n",
        "    .schema(schema) \\\n",
        "    .option(\"sep\", \",\") \\\n",
        "    .csv(\"/tmp/stream_input\")  # папка, куда будут поступать новые файлы\n",
        "\n",
        "# Загружаем модель и трансформеры\n",
        "model = LogisticRegressionModel.load(\"/tmp/lr_model\")\n",
        "indexer_model = StringIndexerModel.from_labels([\"A\", \"B\"], inputCol=\"category\", outputCol=\"category_indexed\")\n",
        "assembler = VectorAssembler(inputCols=[\"category_indexed\", \"feature1\"], outputCol=\"features\")\n",
        "\n",
        "# Применяем предобработку и модель\n",
        "indexed_df = indexer_model.transform(stream_df)\n",
        "features_df = assembler.transform(indexed_df)\n",
        "predictions = model.transform(features_df)\n",
        "\n",
        "# Выводим id, вероятность и предсказанную метку\n",
        "output_df = predictions.select(\"id\", \"probability\", \"prediction\")\n",
        "\n",
        "# Пишем в консоль (можно заменить на Parquet, Kafka и т.д.)\n",
        "query = output_df.writeStream \\\n",
        "    .format(\"console\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()\n"
      ],
      "metadata": {
        "id": "LQNcYv3WVQbI"
      },
      "id": "LQNcYv3WVQbI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2RQQTeA5VQY1"
      },
      "id": "2RQQTeA5VQY1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fqhoLlffVQWs"
      },
      "id": "fqhoLlffVQWs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pv4j_9mIVQUi"
      },
      "id": "Pv4j_9mIVQUi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YVETQtd9VQSj"
      },
      "id": "YVETQtd9VQSj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "enDp5d2cVQQf"
      },
      "id": "enDp5d2cVQQf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MUBqRWy7VQNt"
      },
      "id": "MUBqRWy7VQNt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H9bXJnNnVP1R"
      },
      "id": "H9bXJnNnVP1R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n5eib9R0U_it"
      },
      "id": "n5eib9R0U_it",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rwB9miiDU_gl"
      },
      "id": "rwB9miiDU_gl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3_qhsGMhU_eT"
      },
      "id": "3_qhsGMhU_eT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}